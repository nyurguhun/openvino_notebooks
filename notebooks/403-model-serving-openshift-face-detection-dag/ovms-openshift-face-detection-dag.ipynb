{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0945e553",
   "metadata": {},
   "source": [
    "# Face Detection Multi-Model OpenVINO Model Server Deployment in OpenShift\n",
    "\n",
    "We will show you how to deploy OpenVINO Model Server (OVMS) service with multiple models in an OpenShift cluster. We will run a face detection request to the AI inference service which will return age, gender, and emotion recognition as an ouput for each detected face.\n",
    "\n",
    "Requirements:\n",
    "- OpenShift cluster with the API access to a project\n",
    "- installed [OpenVINO Model Server Operator](https://catalog.redhat.com/software/operators/search?q=openvino)\n",
    "- JupyterLab environment with Python3 deployed in the cluster\n",
    "\n",
    "If you don't have an OpenShift account, you can sign up for 30 or 60 day [free trial of Red Hat OpenShift](https://www.openshift.com/try)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820dd33a",
   "metadata": {},
   "source": [
    "## Login to OpenShift with API Token\n",
    "\n",
    "First, let's login to OpenShift cluster using `oc` tool. \n",
    "\n",
    "In the Red Hat OpenShift console, click on your username and select `Copy login command`.\n",
    "\n",
    "![copy-login.png](notebook-files/copy-login.png)\n",
    "\n",
    "Click on `Display Token` and your API token will appear.\n",
    "\n",
    "![log-in-with-token.png](notebook-files/log-in-with-token.png)\n",
    "\n",
    "Copy `Log in with token` command and paste it in the cell below. The command has your `<user-API-token>` and `<cluster-DNS-name>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dec1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!oc login --token=<user-API-token> --server=https://api.<cluster-DNS-name>:6443"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe95716c",
   "metadata": {},
   "source": [
    "Create `ovms` project and go to this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d059fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!oc new-project ovms\n",
    "!oc project ovms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6328b8ef",
   "metadata": {},
   "source": [
    "## Create MinIO Storage\n",
    "\n",
    "OpenVINO Model Server exposes DL models over gRPC and REST interface. The models can be stored in cloud storage like AWS S3, Google Storage or Azure Blobs. In OpenShift and Kubernetes, Persistent Storage Claim could be used as well. In this tutorial, we will use MinIO service which is an equivalent of AWS S3.\n",
    "\n",
    "Let's create a MinIO service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca08f21",
   "metadata": {},
   "source": [
    "Now deploy Minio service. Note that the configuration below creates Minio server with emphemeral storage which will be deleted each time the pod is restarted. It includes also the default credentials. All in all, it is only a demonstrative purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfecbd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!oc apply -f minio.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46428bd9",
   "metadata": {},
   "source": [
    "Next step is to download `mc`, MinIO Client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d564d1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dl.min.io/client/mc/release/linux-amd64/mc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552821d9",
   "metadata": {},
   "source": [
    "Change the access permissions on `mc`, so we can run commands with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e082a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 755 mc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c985f7e",
   "metadata": {},
   "source": [
    "Let's make an alias for the MinIO service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d73ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./mc alias set minio http://minio-service.ovms:9000 minio minio123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541ec9e",
   "metadata": {},
   "source": [
    "Create a `minio/models` bucket; it's where we will store our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a2f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./mc mb minio/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c26fb1",
   "metadata": {},
   "source": [
    "## Create Model Repository\n",
    "\n",
    "Now, we will upload the models to the MinIO bucket for serving in the OpenVINO Model Server. We will use 3 models:\n",
    "* [face detection](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/intel/face-detection-retail-0004/description/face-detection-retail-0004.md)\n",
    "* [age gender recognition](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/intel/age-gender-recognition-retail-0013/description/age-gender-recognition-retail-0013.md)\n",
    "* [emotion recognition](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/intel/emotions-recognition-retail-0003/description/emotions-recognition-retail-0003.md)\n",
    "\n",
    "First, we will download the models here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c556a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013.xml -o age-gender/1/age-gender-recognition-retail-0013.xml \n",
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013.bin -o age-gender/1/age-gender-recognition-retail-0013.bin\n",
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/face-detection-retail-0004/FP32/face-detection-retail-0004.xml -o face-detection/1/face-detection-retail-0004.xml\n",
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/face-detection-retail-0004/FP32/face-detection-retail-0004.bin -o face-detection/1/face-detection-retail-0004.bin\n",
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/emotions-recognition-retail-0003/FP32/emotions-recognition-retail-0003.xml -o emotions/1/emotions-recognition-retail-0003.xml\n",
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/emotions-recognition-retail-0003/FP32/emotions-recognition-retail-0003.bin -o emotions/1/emotions-recognition-retail-0003.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adddb5a6",
   "metadata": {},
   "source": [
    "Now, copy the models into the MinIO bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2909951",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./mc cp --recursive age-gender minio/models/\n",
    "!./mc cp --recursive face-detection minio/models/\n",
    "!./mc cp --recursive emotions minio/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe46fef4",
   "metadata": {},
   "source": [
    "Let's make sure the models have been successfully copied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62594f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./mc ls -r minio/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c18bb37",
   "metadata": {},
   "source": [
    "## Create a Directed Acyclic Graph Pipeline\n",
    "\n",
    "To implement the multi-model pipeline, we will use a Directed Acyclic Graph, or DAG, scheduler. Here's the workflow.\n",
    "\n",
    "![graph](notebook-files/faces_analysis_graph.svg)\n",
    "\n",
    "As you can see from the workflow, DAG is a graph that doesn't have any loops. It consists of processes that are only moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b904ba",
   "metadata": {},
   "source": [
    "We will need the `config.json` to define the DAG pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a230183",
   "metadata": {},
   "source": [
    "## Create a Custom Node\n",
    "\n",
    "We will use [this custom node](https://github.com/openvinotoolkit/model_server/tree/develop/src/custom_nodes/model_zoo_intel_object_detection) which will analyze the response from the face detection model. Based on the inference results and the input image, the custom node will generate a list of detected boxes. Each image in the output will be resized to the predefined target size to fit the input of the next model in the DAG pipeline. In addition to detected boxes, the results include the coordinates and the detection scores. \n",
    "\n",
    "The main functionality of custom node in the OVMS DAG scheduler is that it allows us to create an arbitrary implementation of the data transformation node in the pipeline. It will be attached to the Model Server as a dynamic library. \n",
    "\n",
    "Clone the Model Server repo and download OpenCV archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a214cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --depth=1 -b develop https://github.com/openvinotoolkit/model_server\n",
    "!curl -s https://download.01.org/opencv/master/openvinotoolkit/thirdparty/linux/opencv/opencv_4.5.1-044_centos7.txz | tar --use-compress-program=xz -xf -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894459af",
   "metadata": {},
   "source": [
    "Run commands below to compile `libcustom_node.so`, the custom node library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -c -std=c++17 model_server/src/custom_nodes/model_zoo_intel_object_detection/model_zoo_intel_object_detection.cpp -fpic  -I./opencv/include/ -Wall -Wno-unknown-pragmas -Werror -fno-strict-overflow -fno-delete-null-pointer-checks -fwrapv -fstack-protector\n",
    "!g++ -shared -o libcustom_node.so model_zoo_intel_object_detection.o -L./opencv/lib/ -I./opencv/include/ -lopencv_core -lopencv_imgproc -lopencv_imgcodecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2c6ff",
   "metadata": {},
   "source": [
    "Let's check if the custom node library was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54857ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l libcustom_node.so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca9cc24",
   "metadata": {},
   "source": [
    "## Deploy OpenVINO Model Server with a Multi-Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a3a6fa",
   "metadata": {},
   "source": [
    "Let's add the custom node library and `config.json` to a ConfigMap resource. Later, it will be mounted inside the OVMS service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!oc create configmap ovms-face-detection-pipeline \\\n",
    "                    --from-file=libcustom_node.so=libcustom_node.so \\\n",
    "                    --from-file=config.json=config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4d8cf",
   "metadata": {},
   "source": [
    "Here's the yaml file used to configure the OVMS service. We specified its name to be `ovms-pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12fd4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ovms-face-detection-pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3598b8b",
   "metadata": {},
   "source": [
    "Let's create the `ovms-pipeline` service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4d54b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!oc apply -f ovms-face-detection-pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5099f9d2",
   "metadata": {},
   "source": [
    "Let's see if pod and service were created. They should start with `ovms-pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!oc get pod\n",
    "!oc get service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68145557",
   "metadata": {},
   "source": [
    "Let's check if the OpenVINO Model Server service is running by making an API request via cURL. Models' `state` should be `AVAILABLE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8facc30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s http://ovms-pipeline.ovms.svc:8081/v1/config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8799a48",
   "metadata": {},
   "source": [
    "## Run an Inference Request\n",
    "\n",
    "The pipeline execution is represented as the `find_face_images` model. The client runs an inference request exactly the same way as it would run with a single model.\n",
    "\n",
    "Let's import Python packages needed for inference request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2db064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import make_tensor_proto, make_ndarray\n",
    "import argparse\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae8c2b9",
   "metadata": {},
   "source": [
    "We will run face detection on this image.\n",
    "\n",
    "![people](people.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4687bdc",
   "metadata": {},
   "source": [
    "Prepare the image for sending it in the gRPC request to the `ovms-pipeline` service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc42dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('people.jpg').astype(np.float32)  # BGR color format, shape HWC\n",
    "resolution = (400, 600)\n",
    "img = cv2.resize(img, (resolution[1], resolution[0]))\n",
    "img = img.transpose(2,0,1).reshape(1,3,resolution[0],resolution[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa05766",
   "metadata": {},
   "source": [
    "Next, let's establish connection with the `ovms-pipeline` service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304fd3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "address = \"ovms-pipeline.ovms.svc:8080\"\n",
    "MAX_MESSAGE_LENGTH = 1024 * 1024 * 8  # incresed default max size of the message\n",
    "channel = grpc.insecure_channel(address,\n",
    "    options=[\n",
    "        ('grpc.max_send_message_length', MAX_MESSAGE_LENGTH),\n",
    "        ('grpc.max_receive_message_length', MAX_MESSAGE_LENGTH),\n",
    "    ])\n",
    "\n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "request = predict_pb2.PredictRequest()\n",
    "request.model_spec.name = \"find_face_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7d320c",
   "metadata": {},
   "source": [
    "Send the request and prediction will be executed. Note that the exception is handled when the pipeline doesn't detect any face in the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7933ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "request.inputs['image'].CopyFrom(make_tensor_proto(img, shape=img.shape))\n",
    "try:\n",
    "    response = stub.Predict(request, 10.0)\n",
    "except grpc.RpcError as err:\n",
    "    if err.code() == grpc.StatusCode.ABORTED:\n",
    "        print('No face has been found in the image')\n",
    "        exit(1)\n",
    "    else:\n",
    "        raise err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c880aa16",
   "metadata": {},
   "source": [
    "We will receive results as `ages`, `genders`, `emotions`, and `face_coordinates` in `response` object. `face_images` output returns cropped faces retrieved from the original image.\n",
    "\n",
    "Define functions to process the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b4da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "def save_face_images_as_jpgs(output_nd, name, location):\n",
    "    for i in range(output_nd.shape[0]):\n",
    "        out = output_nd[i][0]\n",
    "        out = out.transpose(1,2,0)\n",
    "        output_file_name = name + '_' + str(i) + '.jpg'\n",
    "        cv2.imwrite(os.path.join(location, output_file_name), out)\n",
    "        images.append(output_file_name)\n",
    "        \n",
    "def update_people_ages(output_nd, people):\n",
    "    for i in range(output_nd.shape[0]):\n",
    "        age = int(output_nd[i,0,0,0,0] * 100)\n",
    "        if len(people) < i + 1:\n",
    "            people.append({'age': age})\n",
    "        else:\n",
    "            people[i].update({'age': age})\n",
    "    return people\n",
    "\n",
    "def update_people_genders(output_nd, people):\n",
    "    for i in range(output_nd.shape[0]):\n",
    "        gender = 'male' if output_nd[i,0,0,0,0] < output_nd[i,0,1,0,0] else 'female'\n",
    "        if len(people) < i + 1:\n",
    "            people.append({'gender': gender})\n",
    "        else:\n",
    "            people[i].update({'gender': gender})\n",
    "    return people\n",
    "\n",
    "def update_people_emotions(output_nd, people):\n",
    "    emotion_names = {\n",
    "        0: 'neutral',\n",
    "        1: 'happy',\n",
    "        2: 'sad',\n",
    "        3: 'surprised',\n",
    "        4: 'angry'\n",
    "    }\n",
    "    for i in range(output_nd.shape[0]):\n",
    "        emotion_id = np.argmax(output_nd[i,0,:,0,0])\n",
    "        emotion = emotion_names[emotion_id]\n",
    "        if len(people) < i + 1:\n",
    "            people.append({'emotion': emotion})\n",
    "        else:\n",
    "            people[i].update({'emotion': emotion})\n",
    "    return people\n",
    "\n",
    "def update_people_coordinate(output_nd, people):\n",
    "    for i in range(output_nd.shape[0]):\n",
    "        if len(people) < i + 1:\n",
    "            people.append({'coordinate': output_nd[i,0,:]})\n",
    "        else:\n",
    "            people[i].update({'coordinate': output_nd[i,0,:]})\n",
    "    return people"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b740dacc",
   "metadata": {},
   "source": [
    "Let's process the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ffeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "people = []\n",
    "\n",
    "for name in response.outputs:\n",
    "    print(f\"Output: name[{name}]\")\n",
    "    tensor_proto = response.outputs[name]\n",
    "    output_nd = make_ndarray(tensor_proto)\n",
    "    print(f\"    numpy => shape[{output_nd.shape}] data[{output_nd.dtype}]\")\n",
    "\n",
    "    if name == 'face_images':\n",
    "        save_face_images_as_jpgs(output_nd, name, \".\")\n",
    "    if name == 'ages':\n",
    "        people = update_people_ages(output_nd, people)\n",
    "    if name == 'genders':\n",
    "        people = update_people_genders(output_nd, people)\n",
    "    if name == 'emotions':\n",
    "        people = update_people_emotions(output_nd, people)\n",
    "    if name == 'face_coordinates':\n",
    "        people = update_people_coordinate(output_nd, people)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03503895",
   "metadata": {},
   "source": [
    "Now we can view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152bd1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nFound', len(people), 'faces:\\n')\n",
    "for num, person in enumerate(people):\n",
    "    display(Image(images[num]))\n",
    "    print(f\"\"\"\n",
    "Estimated Age: {person['age']}\n",
    "Estimated Gender: {person['gender']}\n",
    "Estimated Emotion: {person['emotion']}\n",
    "Original Image Coordinates:{person['coordinate']}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b86b99",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Let's free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0445a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!oc delete ovms ovms-pipeline\n",
    "!oc delete deploy minio\n",
    "!oc delete service minio-service\n",
    "!oc delete configmap ovms-face-detection-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7077ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf age-gender\n",
    "!rm -rf emotions\n",
    "!rm -rf face-detection\n",
    "!rm mc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a757be36",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In this notebook, you have learned how to deploy an OVMS service with multiple models in an OpenShift cluster. Next, you can explore other OVMS notebooks:\n",
    "\n",
    "- [Deploy Image Classification with OpenVINO Model Server in OpenShift](../401-model-serving-openshift-resnet/ovms-openshift-resnet.ipynb)\n",
    "- [Send gRPC and API Calls via Python Scripts to OpenVINO Model Server in OpenShift](../402-model-serving-openshift-python-scripts/ovms-openshift-python-scripts.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
