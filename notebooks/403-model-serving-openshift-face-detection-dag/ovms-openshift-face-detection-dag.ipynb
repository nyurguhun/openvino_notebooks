{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "394b97e9",
   "metadata": {},
   "source": [
    "# Face Detection Multi-Model OpenVINO Model Server Deployment in OpenShift\n",
    "\n",
    "We will show you how to deploy OpenVINO Model Server (OVMS) service with multiple models in an OpenShift cluster. We will run a face detection request to the AI inference service which will return age, gender, and emotion recognition as an ouput for each detected face.\n",
    "\n",
    "Requirements:\n",
    "- OpenShift cluster with the API access to a project\n",
    "- installed [OpenVINO Model Server Operator](https://catalog.redhat.com/software/operators/search?q=openvino)\n",
    "- JupyterLab environment with Python3 deployed in the cluster\n",
    "\n",
    "If you don't have an OpenShift account, you can sign up for 30 or 60 day [free trial of Red Hat OpenShift](https://www.openshift.com/try)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce0c403",
   "metadata": {},
   "source": [
    "## Login to OpenShift with API Token\n",
    "\n",
    "First, let's login to OpenShift cluster using `oc` tool. \n",
    "\n",
    "In the Red Hat OpenShift console, click on your username and select `Copy login command`.\n",
    "\n",
    "![copy-login.png](notebook-files/copy-login.png)\n",
    "\n",
    "Click on `Display Token` and your API token will appear.\n",
    "\n",
    "![log-in-with-token.png](notebook-files/log-in-with-token.png)\n",
    "\n",
    "Copy `Log in with token` command and paste it in the cell below. The command has your `<user-API-token>` and `<cluster-DNS-name>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95b2eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged into \"https://api.openvino5.3q12.p1.openshiftapps.com:6443\" as \"nyurguhun\" using the token provided.\n",
      "\n",
      "You have access to the following projects and can switch between them with 'oc project <projectname>':\n",
      "\n",
      "  * ovms\n",
      "    rose-jh\n",
      "\n",
      "Using project \"ovms\".\n"
     ]
    }
   ],
   "source": [
    "!oc login --token=<user-API-token> --server=https://api.<cluster-DNS-name>:6443"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ad0c9a",
   "metadata": {},
   "source": [
    "Create `ovms` project and go to this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06092b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (AlreadyExists): project.project.openshift.io \"ovms\" already exists\n",
      "Already on project \"ovms\" on server \"https://api.openvino5.3q12.p1.openshiftapps.com:6443\".\n"
     ]
    }
   ],
   "source": [
    "!oc new-project ovms\n",
    "!oc project ovms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506a77b",
   "metadata": {},
   "source": [
    "## Create MinIO Storage\n",
    "\n",
    "OpenVINO Model Server exposes DL models over gRPC and REST interface. The models can be stored in cloud storage like AWS S3, Google Storage or Azure Blobs. In OpenShift and Kubernetes, Persistent Storage Claim could be used as well. In this tutorial, we will use MinIO service which is an equivalent of AWS S3.\n",
    "\n",
    "Let's create a MinIO service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39f1259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/minio created\n",
      "service/minio-service created\n"
     ]
    }
   ],
   "source": [
    "!oc apply -f minio.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6285d85",
   "metadata": {},
   "source": [
    "Next step is to download `mc`, MinIO Client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f23af9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-12 16:33:41--  https://dl.min.io/client/mc/release/linux-amd64/mc\n",
      "Resolving dl.min.io (dl.min.io)... 178.128.69.202\n",
      "Connecting to dl.min.io (dl.min.io)|178.128.69.202|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20819968 (20M) [application/octet-stream]\n",
      "Saving to: ‘mc’\n",
      "\n",
      "mc                  100%[===================>]  19.86M  14.3MB/s    in 1.4s    \n",
      "\n",
      "2021-05-12 16:33:42 (14.3 MB/s) - ‘mc’ saved [20819968/20819968]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.min.io/client/mc/release/linux-amd64/mc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba312d1",
   "metadata": {},
   "source": [
    "Change the access permissions on `mc`, so we can run commands with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdbe0f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 755 mc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4844f1",
   "metadata": {},
   "source": [
    "Let's make an alias for the MinIO service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d48c0e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[32mAdded `minio` successfully.\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!./mc alias set minio http://minio-service.ovms:9000 minio minio123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc44d6",
   "metadata": {},
   "source": [
    "Create a `minio/models` bucket; it's where we will store our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829903b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[32;1mBucket created successfully `minio/models`.\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!./mc mb minio/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf18acb",
   "metadata": {},
   "source": [
    "## Create Model Repository\n",
    "\n",
    "Now, we will upload the models to the MinIO bucket for serving in the OpenVINO Model Server. We will use 3 models:\n",
    "* [face detection](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/intel/face-detection-retail-0004/description/face-detection-retail-0004.md)\n",
    "* [age gender recognition](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/intel/age-gender-recognition-retail-0013/description/age-gender-recognition-retail-0013.md)\n",
    "* [emotion recognition](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/intel/emotions-recognition-retail-0003/description/emotions-recognition-retail-0003.md)\n",
    "\n",
    "First, we will download the models here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8be7f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 30901  100 30901    0     0  54307      0 --:--:-- --:--:-- --:--:-- 54307\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 8351k  100 8351k    0     0  11.1M      0 --:--:-- --:--:-- --:--:-- 11.1M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  101k  100  101k    0     0   711k      0 --:--:-- --:--:-- --:--:--  711k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2297k  100 2297k    0     0  2671k      0 --:--:-- --:--:-- --:--:-- 2668k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 39391  100 39391    0     0  98231      0 --:--:-- --:--:-- --:--:-- 97987\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 9697k  100 9697k    0     0  9706k      0 --:--:-- --:--:-- --:--:-- 9697k\n"
     ]
    }
   ],
   "source": [
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013.xml -o age-gender/1/age-gender-recognition-retail-0013.xml \n",
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/age-gender-recognition-retail-0013/FP32/age-gender-recognition-retail-0013.bin -o age-gender/1/age-gender-recognition-retail-0013.bin\n",
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/face-detection-retail-0004/FP32/face-detection-retail-0004.xml -o face-detection/1/face-detection-retail-0004.xml\n",
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/face-detection-retail-0004/FP32/face-detection-retail-0004.bin -o face-detection/1/face-detection-retail-0004.bin\n",
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/emotions-recognition-retail-0003/FP32/emotions-recognition-retail-0003.xml -o emotions/1/emotions-recognition-retail-0003.xml\n",
    "!curl --create-dirs https://storage.openvinotoolkit.org/repositories/open_model_zoo/2021.3/models_bin/2/emotions-recognition-retail-0003/FP32/emotions-recognition-retail-0003.bin -o emotions/1/emotions-recognition-retail-0003.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9f7d2b",
   "metadata": {},
   "source": [
    "Now, copy the models into the MinIO bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f81e8efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...-0003.xml:  9.51 MiB / 9.51 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 175.37 MiB/s 0s\u001b[0m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m"
     ]
    }
   ],
   "source": [
    "!./mc cp --recursive age-gender minio/models/\n",
    "!./mc cp --recursive face-detection minio/models/\n",
    "!./mc cp --recursive emotions minio/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce62b59b",
   "metadata": {},
   "source": [
    "Let's make sure the models have been successfully copied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a6fa1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[32m[2021-05-12 16:33:55 UTC]\u001b[0m\u001b[33m 8.2MiB\u001b[0m\u001b[1m age-gender/1/age-gender-recognition-retail-0013.bin\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2021-05-12 16:33:55 UTC]\u001b[0m\u001b[33m  30KiB\u001b[0m\u001b[1m age-gender/1/age-gender-recognition-retail-0013.xml\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2021-05-12 16:33:56 UTC]\u001b[0m\u001b[33m 9.5MiB\u001b[0m\u001b[1m emotions/1/emotions-recognition-retail-0003.bin\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2021-05-12 16:33:56 UTC]\u001b[0m\u001b[33m  38KiB\u001b[0m\u001b[1m emotions/1/emotions-recognition-retail-0003.xml\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2021-05-12 16:33:56 UTC]\u001b[0m\u001b[33m 2.2MiB\u001b[0m\u001b[1m face-detection/1/face-detection-retail-0004.bin\u001b[0m\n",
      "\u001b[0m\u001b[m\u001b[32m[2021-05-12 16:33:56 UTC]\u001b[0m\u001b[33m 102KiB\u001b[0m\u001b[1m face-detection/1/face-detection-retail-0004.xml\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!./mc ls -r minio/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e97fea",
   "metadata": {},
   "source": [
    "## Create a Directed Acyclic Graph Pipeline\n",
    "\n",
    "To implement the multi-model pipeline, we will use a Directed Acyclic Graph, or DAG, scheduler. Here's the workflow.\n",
    "\n",
    "![graph](notebook-files/faces_analysis_graph.svg)\n",
    "\n",
    "As you can see from the workflow, DAG is a graph that doesn't have any loops. It consists of processes that are only moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe2696",
   "metadata": {},
   "source": [
    "We will need the `config.json` to define the DAG pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ede0bfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model_config_list\": [\n",
      "    {\"config\": {\n",
      "      \"name\": \"face_detection\",\n",
      "      \"base_path\": \"s3://models/face-detection/\",\n",
      "      \"shape\": \"(1,3,400,600)\"}},\n",
      "    {\"config\": {\n",
      "      \"name\": \"age_gender_recognition\",\n",
      "      \"base_path\": \"s3://models/age-gender/\",\n",
      "      \"shape\": \"(1,3,64,64)\"}},\n",
      "    {\"config\": {\n",
      "      \"name\": \"emotion_recognition\",\n",
      "      \"base_path\": \"s3://models/emotions/\",\n",
      "      \"shape\": \"(1,3,64,64)\"}}\n",
      "  ],\n",
      "  \"custom_node_library_config_list\": [\n",
      "    {\"name\": \"object_detection_image_extractor\",\n",
      "      \"base_path\": \"/config/libcustom_node.so\"}\n",
      "  ],\n",
      "  \"pipeline_config_list\": [\n",
      "    {\n",
      "      \"name\": \"find_face_images\",\n",
      "      \"inputs\": [\n",
      "        \"image\"\n",
      "      ],\n",
      "      \"nodes\": [\n",
      "        {\n",
      "          \"name\": \"face_detection_node\",\n",
      "          \"model_name\": \"face_detection\",\n",
      "          \"type\": \"DL model\",\n",
      "          \"inputs\": [\n",
      "            {\"data\": {\n",
      "              \"node_name\": \"request\",\n",
      "              \"data_item\": \"image\"}}],\n",
      "          \"outputs\": [\n",
      "            {\"data_item\": \"detection_out\",\n",
      "              \"alias\": \"detection\"}]\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"extract_node\",\n",
      "          \"library_name\": \"object_detection_image_extractor\",\n",
      "          \"type\": \"custom\",\n",
      "          \"demultiply_count\": 0,\n",
      "          \"params\": {\n",
      "            \"original_image_width\": \"600\",\n",
      "            \"original_image_height\": \"400\",\n",
      "            \"target_image_width\": \"64\",\n",
      "            \"target_image_height\": \"64\",\n",
      "            \"convert_to_gray_scale\": \"false\",\n",
      "            \"max_output_batch\": \"100\",\n",
      "            \"confidence_threshold\": \"0.7\",\n",
      "            \"debug\": \"true\"\n",
      "          },\n",
      "          \"inputs\": [\n",
      "            {\"image\": {\n",
      "              \"node_name\": \"request\",\n",
      "              \"data_item\": \"image\"}},\n",
      "            {\"detection\": {\n",
      "              \"node_name\": \"face_detection_node\",\n",
      "              \"data_item\": \"detection\"}}],\n",
      "          \"outputs\": [\n",
      "            {\"data_item\": \"images\",\n",
      "              \"alias\": \"face_images\"},\n",
      "            {\"data_item\": \"coordinates\",\n",
      "              \"alias\": \"face_coordinates\"},\n",
      "            {\"data_item\": \"confidences\",\n",
      "              \"alias\": \"confidence_levels\"}]\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"age_gender_recognition_node\",\n",
      "          \"model_name\": \"age_gender_recognition\",\n",
      "          \"type\": \"DL model\",\n",
      "          \"inputs\": [\n",
      "            {\"data\": {\n",
      "              \"node_name\": \"extract_node\",\n",
      "              \"data_item\": \"face_images\"}}],\n",
      "          \"outputs\": [\n",
      "            {\"data_item\": \"age_conv3\",\n",
      "              \"alias\": \"age\"},\n",
      "            {\"data_item\": \"prob\",\n",
      "              \"alias\": \"gender\"}]\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"emotion_recognition_node\",\n",
      "          \"model_name\": \"emotion_recognition\",\n",
      "          \"type\": \"DL model\",\n",
      "          \"inputs\": [\n",
      "            {\"data\": {\n",
      "              \"node_name\": \"extract_node\",\n",
      "              \"data_item\": \"face_images\"}}],\n",
      "          \"outputs\": [\n",
      "            {\"data_item\": \"prob_emotion\",\n",
      "              \"alias\": \"emotion\"}]\n",
      "        }\n",
      "      ],\n",
      "      \"outputs\": [\n",
      "        {\"face_images\": {\n",
      "          \"node_name\": \"extract_node\",\n",
      "          \"data_item\": \"face_images\"}},\n",
      "        {\"face_coordinates\": {\n",
      "          \"node_name\": \"extract_node\",\n",
      "          \"data_item\": \"face_coordinates\"}},\n",
      "        {\"confidence_levels\": {\n",
      "          \"node_name\": \"extract_node\",\n",
      "          \"data_item\": \"confidence_levels\"}},\n",
      "        {\"ages\": {\n",
      "          \"node_name\": \"age_gender_recognition_node\",\n",
      "          \"data_item\": \"age\"}},\n",
      "        {\"genders\": {\n",
      "          \"node_name\": \"age_gender_recognition_node\",\n",
      "          \"data_item\": \"gender\"}},\n",
      "        {\"emotions\": {\n",
      "          \"node_name\": \"emotion_recognition_node\",\n",
      "          \"data_item\": \"emotion\"}}\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d6691",
   "metadata": {},
   "source": [
    "## Create a Custom Node\n",
    "\n",
    "We will use [this custom node](https://github.com/openvinotoolkit/model_server/tree/develop/src/custom_nodes/model_zoo_intel_object_detection) which will analyze the response from the face detection model. Based on the inference results and the input image, the custom node will generate a list of detected boxes. Each image in the output will be resized to the predefined target size to fit the input of the next model in the DAG pipeline. In addition to detected boxes, the results include the coordinates and the detection scores. \n",
    "\n",
    "The main functionality of custom node in the OVMS DAG scheduler is that it allows us to create an arbitrary implementation of the data transformation node in the pipeline. It will be attached to the Model Server as a dynamic library. \n",
    "\n",
    "Clone the Model Server repo and download OpenCV archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d904891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'model_server'...\n",
      "remote: Enumerating objects: 588, done.\u001b[K\n",
      "remote: Counting objects: 100% (588/588), done.\u001b[K\n",
      "remote: Compressing objects: 100% (508/508), done.\u001b[K\n",
      "remote: Total 588 (delta 157), reused 272 (delta 60), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (588/588), 4.43 MiB | 40.88 MiB/s, done.\n",
      "Resolving deltas: 100% (157/157), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth=1 -b develop https://github.com/openvinotoolkit/model_server\n",
    "!curl -s https://download.01.org/opencv/master/openvinotoolkit/thirdparty/linux/opencv/opencv_4.5.1-044_centos7.txz | tar --use-compress-program=xz -xf -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc32c73f",
   "metadata": {},
   "source": [
    "Run commands below to compile `libcustom_node.so`, the custom node library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed1d8bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -c -std=c++17 model_server/src/custom_nodes/model_zoo_intel_object_detection/model_zoo_intel_object_detection.cpp -fpic  -I./opencv/include/ -Wall -Wno-unknown-pragmas -Werror -fno-strict-overflow -fno-delete-null-pointer-checks -fwrapv -fstack-protector\n",
    "!g++ -shared -o libcustom_node.so model_zoo_intel_object_detection.o -L./opencv/lib/ -I./opencv/include/ -lopencv_core -lopencv_imgproc -lopencv_imgcodecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb7397",
   "metadata": {},
   "source": [
    "Let's check if the custom node library was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1d42552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxr-xr-x. 1 1000890000 1000890000 212104 May 12 16:34 libcustom_node.so\n"
     ]
    }
   ],
   "source": [
    "!ls -l libcustom_node.so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f40c52",
   "metadata": {},
   "source": [
    "## Deploy OpenVINO Model Server with a Multi-Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eee145",
   "metadata": {},
   "source": [
    "Let's add the custom node library and `config.json` to a ConfigMap resource. Later, it will be mounted inside the OVMS service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dae185b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap/ovms-face-detection-pipeline created\n"
     ]
    }
   ],
   "source": [
    "!oc create configmap ovms-face-detection-pipeline \\\n",
    "                    --from-file=libcustom_node.so=libcustom_node.so \\\n",
    "                    --from-file=config.json=config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7a9e97",
   "metadata": {},
   "source": [
    "Here's the yaml file used to configure the OVMS service. We specified its name to be `ovms-pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54e7324a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: intel.com/v1alpha1\n",
      "kind: Ovms\n",
      "metadata:\n",
      "  name: ovms-pipeline\n",
      "  namespace: ovms\n",
      "spec:\n",
      "  aws_access_key_id: \"minio\"\n",
      "  aws_region: \"us-east-1\"\n",
      "  aws_secret_access_key: \"minio123\"\n",
      "  s3_compat_api_endpoint: 'http://minio-service:9000'\n",
      "  config_configmap_name: 'ovms-face-detection-pipeline'\n",
      "  grpc_port: 8080\n",
      "  image_name: >-\n",
      "    registry.connect.redhat.com/intel/openvino-model-server:latest\n",
      "  log_level: INFO\n",
      "  plugin_config: '{\\\"CPU_THROUGHPUT_STREAMS\\\":\\\"1\\\"}'\n",
      "  replicas: 1\n",
      "  rest_port: 8081\n",
      "  service_type: ClusterIP\n"
     ]
    }
   ],
   "source": [
    "!cat ovms-face-detection-pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c9dfd5",
   "metadata": {},
   "source": [
    "Let's create the `ovms-pipeline` service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "002db38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ovms.intel.com/ovms-pipeline created\n"
     ]
    }
   ],
   "source": [
    "!oc apply -f ovms-face-detection-pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be373d",
   "metadata": {},
   "source": [
    "It takes around 15 seconds to create the pod and the service. They should start with `ovms-pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8780a999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                             READY     STATUS    RESTARTS   AGE\n",
      "minio-5c57f888dd-vpdcd           1/1       Running   0          65s\n",
      "ovms-pipeline-66494fbd96-mdjxj   1/1       Running   0          26s\n",
      "NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\n",
      "minio-service   ClusterIP   172.30.39.13    <none>        9000/TCP            66s\n",
      "ovms-pipeline   ClusterIP   172.30.91.199   <none>        8080/TCP,8081/TCP   27s\n"
     ]
    }
   ],
   "source": [
    "!oc get pod\n",
    "!oc get service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798339e8",
   "metadata": {},
   "source": [
    "Let's check if the OpenVINO Model Server service is running by making an API request via cURL. Models' `state` should be `AVAILABLE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2894dd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"age_gender_recognition\" : \n",
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"OK\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "},\n",
      "\"emotion_recognition\" : \n",
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"OK\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "},\n",
      "\"face_detection\" : \n",
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"OK\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "},\n",
      "\"find_face_images\" : \n",
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"OK\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "}\n",
      "}"
     ]
    }
   ],
   "source": [
    "!curl -s http://ovms-pipeline.ovms.svc:8081/v1/config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c1283",
   "metadata": {},
   "source": [
    "## Run an Inference Request\n",
    "\n",
    "The pipeline execution is represented as the `find_face_images` model. The client runs an inference request exactly the same way as it would run with a single model.\n",
    "\n",
    "Let's import Python packages needed for inference request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98a5719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import make_tensor_proto, make_ndarray\n",
    "import argparse\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d16a32",
   "metadata": {},
   "source": [
    "We will run face detection on this image.\n",
    "\n",
    "![people](people.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4442a039",
   "metadata": {},
   "source": [
    "Prepare the image for sending it in the gRPC request to the `ovms-pipeline` service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ff16972",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('people.jpg').astype(np.float32)  # BGR color format, shape HWC\n",
    "resolution = (400, 600)\n",
    "img = cv2.resize(img, (resolution[1], resolution[0]))\n",
    "img = img.transpose(2,0,1).reshape(1,3,resolution[0],resolution[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed47bc",
   "metadata": {},
   "source": [
    "Next, let's establish connection with the `ovms-pipeline` service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d165e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "address = \"ovms-pipeline.ovms.svc:8080\"\n",
    "MAX_MESSAGE_LENGTH = 1024 * 1024 * 8  # incresed default max size of the message\n",
    "channel = grpc.insecure_channel(address,\n",
    "    options=[\n",
    "        ('grpc.max_send_message_length', MAX_MESSAGE_LENGTH),\n",
    "        ('grpc.max_receive_message_length', MAX_MESSAGE_LENGTH),\n",
    "    ])\n",
    "\n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "request = predict_pb2.PredictRequest()\n",
    "request.model_spec.name = \"find_face_images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a37c71f",
   "metadata": {},
   "source": [
    "Send the request and prediction will be executed. Note that the exception is handled when the pipeline doesn't detect any face in the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48679924",
   "metadata": {},
   "outputs": [],
   "source": [
    "request.inputs['image'].CopyFrom(make_tensor_proto(img, shape=img.shape))\n",
    "try:\n",
    "    response = stub.Predict(request, 10.0)\n",
    "except grpc.RpcError as err:\n",
    "    if err.code() == grpc.StatusCode.ABORTED:\n",
    "        print('No face has been found in the image')\n",
    "        exit(1)\n",
    "    else:\n",
    "        raise err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e3c22",
   "metadata": {},
   "source": [
    "We will receive results as `ages`, `genders`, `emotions`, and `face_coordinates` in `response` object. `face_images` output returns cropped faces retrieved from the original image.\n",
    "\n",
    "Define functions to process the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "408a6a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "def save_face_images_as_jpgs(output_nd, name, location):\n",
    "    for i in range(output_nd.shape[0]):\n",
    "        out = output_nd[i][0]\n",
    "        out = out.transpose(1,2,0)\n",
    "        output_file_name = name + '_' + str(i) + '.jpg'\n",
    "        cv2.imwrite(os.path.join(location, output_file_name), out)\n",
    "        images.append(output_file_name)\n",
    "        \n",
    "def update_people_ages(output_nd, people):\n",
    "    for i in range(output_nd.shape[0]):\n",
    "        age = int(output_nd[i,0,0,0,0] * 100)\n",
    "        if len(people) < i + 1:\n",
    "            people.append({'age': age})\n",
    "        else:\n",
    "            people[i].update({'age': age})\n",
    "    return people\n",
    "\n",
    "def update_people_genders(output_nd, people):\n",
    "    for i in range(output_nd.shape[0]):\n",
    "        gender = 'male' if output_nd[i,0,0,0,0] < output_nd[i,0,1,0,0] else 'female'\n",
    "        if len(people) < i + 1:\n",
    "            people.append({'gender': gender})\n",
    "        else:\n",
    "            people[i].update({'gender': gender})\n",
    "    return people\n",
    "\n",
    "def update_people_emotions(output_nd, people):\n",
    "    emotion_names = {\n",
    "        0: 'neutral',\n",
    "        1: 'happy',\n",
    "        2: 'sad',\n",
    "        3: 'surprised',\n",
    "        4: 'angry'\n",
    "    }\n",
    "    for i in range(output_nd.shape[0]):\n",
    "        emotion_id = np.argmax(output_nd[i,0,:,0,0])\n",
    "        emotion = emotion_names[emotion_id]\n",
    "        if len(people) < i + 1:\n",
    "            people.append({'emotion': emotion})\n",
    "        else:\n",
    "            people[i].update({'emotion': emotion})\n",
    "    return people\n",
    "\n",
    "def update_people_coordinate(output_nd, people):\n",
    "    for i in range(output_nd.shape[0]):\n",
    "        if len(people) < i + 1:\n",
    "            people.append({'coordinate': output_nd[i,0,:]})\n",
    "        else:\n",
    "            people[i].update({'coordinate': output_nd[i,0,:]})\n",
    "    return people"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c65d92",
   "metadata": {},
   "source": [
    "Let's process the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f3c6e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: name[emotions]\n",
      "    numpy => shape[(6, 1, 5, 1, 1)] data[float32]\n",
      "Output: name[ages]\n",
      "    numpy => shape[(6, 1, 1, 1, 1)] data[float32]\n",
      "Output: name[genders]\n",
      "    numpy => shape[(6, 1, 2, 1, 1)] data[float32]\n",
      "Output: name[face_images]\n",
      "    numpy => shape[(6, 1, 3, 64, 64)] data[float32]\n",
      "Output: name[confidence_levels]\n",
      "    numpy => shape[(6, 1, 1)] data[float32]\n",
      "Output: name[face_coordinates]\n",
      "    numpy => shape[(6, 1, 4)] data[float32]\n"
     ]
    }
   ],
   "source": [
    "people = []\n",
    "\n",
    "for name in response.outputs:\n",
    "    print(f\"Output: name[{name}]\")\n",
    "    tensor_proto = response.outputs[name]\n",
    "    output_nd = make_ndarray(tensor_proto)\n",
    "    print(f\"    numpy => shape[{output_nd.shape}] data[{output_nd.dtype}]\")\n",
    "\n",
    "    if name == 'face_images':\n",
    "        save_face_images_as_jpgs(output_nd, name, \".\")\n",
    "    if name == 'ages':\n",
    "        people = update_people_ages(output_nd, people)\n",
    "    if name == 'genders':\n",
    "        people = update_people_genders(output_nd, people)\n",
    "    if name == 'emotions':\n",
    "        people = update_people_emotions(output_nd, people)\n",
    "    if name == 'face_coordinates':\n",
    "        people = update_people_coordinate(output_nd, people)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d6fa6",
   "metadata": {},
   "source": [
    "Now we can view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "630ff168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 6 faces:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated Age: 28\n",
      "Estimated Gender: male\n",
      "Estimated Emotion: happy\n",
      "Original Image Coordinates:[0.3782609  0.35938    0.44495052 0.5501346 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/2wBDAQICAgICAgUDAwUKBwYHCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgr/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD8XtHt59VnNrp0JL5xu5JYD36DFfQX7HX7AHiX9qvxUdNvdfk0uzhOLm5t03vGD3+6Vz/skj9K8b/Z0+GPjH4x/Eyw8B+DNPnvLq8mVCIyTsUnljjooGST6Zr97f2Gv2SPC/7PXwz0zwhYRyXF82HvbuOPrOckgYGcAkgE9qnG4v2K5YvU9DKMr+uSc5r3UfOvwf8A+CEv7KWgAz/ELV/EevSx4ZYLu8WOAt6lY1XcPY/nXulh+xl+zv8ADrQ38P8Ahz4X6RHYOgjltzYJJHKMY+ZXOD+Ofx619R/8K+OnWD3NxoF++xNxEXl7m9SBu5IH5+lcRP8AEn4I6nqEvhjSvF1vFqltkT6XfxvFcAj/AGXAz+FeFXnXq6yZ9ng8PhMPK0In5/8AxA/4JowW3ih/Fn7PPjKXwheB2ydNMpVARghQWJTg9uT0zXj3iz/glp8WfG/iRtU+Kfxa1LxCQ+Ee/vJZHAHGArYI9stx71+pOqa54L1a2NvpcKMBJtZiq4Df57VjXfg3w5qW25m1dUK5UrHKqc+oBPSsvreJjG0Zs7ZZXgKkrypr+vLY/OXUv+CQPgI6SFF3fpcMdzlQvA9OTxXzL8eP2LtR+Bl3dazpwmmtrdiYpQhYqo6scgDjpxX7N+J/CFlZ2ImEscr7TsxMwOPcgV8E/wDBTWXw9D4Zitbm0eOSS7WO5uBFyISpLKH+82Tj5Rxx7U8HjcUsVGPM2mzzszyrL1gpSjBJpb7Fv/ggZ+z7pT6HrfxYubRHu7yf7FEzKrlVQBiy55Gd+DX6N+N9X/aA8E2ckPwa0jQhKifLc61qLwIT6ERwSsPwB/DpXyT/AMEOfB+raJ+zFpuqarp3kJf30k0EksYQum1MMD1K5B/Wv0Iv/EHgvTbDZqmoLHKcL+6gLBsjoCQaWKlOrjJu9tTpymj7HAUklfS/zZ8ReLf+Cmf7ZfwM8VXlh8YPgNNqFnE2yG/8I6ra6gsp9kd4ZAPcruHpXmnxM/bk+Dn7Ud/Brd14U1Twt4osjtV7qyaGSQd1PA3cgcEkHHFfYfxO8GfAz4mXQGs+EpdTukZjFNLawSeSMepXco/3elc58Pv2Rv2Y4vGEEy65FbaszZS1mkjlAU9VSMjcp6fODuHQEA1yVlVqQ5dPW/8Anf8AM9ONJ05cz/8ASV+lvyPDfC/jDxp4d0WPxJ4i1dY1t1ZbS1Fqd8zlcM7jaMcEY9CM1L8OPi38RtY19k8D+EhrF/cSELeXF5thtefvEEgN745HQZr721r9nbwvp3gaSw0y2hSfySInmw3BGDzJuxx/kV8HftP/ALIPxH0R5PEvhq61dYRIRd/8I3exBtvOH8p0IOBwdpyScgVwSoVaE4u+hrTrqvTagfRXgvwd8Xrvwk118Q/ij4WWYnfHD/YkRVG9N2Tn0zxX5w/8FQvhb421fxhL4s8Z69FqFiAY7JdMhEcFsRwSyqfmzgnOARWu/wAFv2krm2Wz8KfG/wAX2MC/KseoabbOT6f66BSOfevN/jr4I/aU0rwyNE8U+N7TWoJJvLaSW1+zyIOc7kXIf/gP5V6VPGxjOLjJfj/kcNbCqph6kakJbeTX4O/4H6Efs9eH7j4SfBzw74StL/bFo+mpbq/kqokAY8gDABOfQUvxY+Ov/CO2TS6lcMAATtYkbvTkdKw/BnxItLnwRbwJqcYdIfv5yG9s9CK8t+Kfh/WfiDqsERdY8PkRA4WQduO9eXjMROOz1bPRwSjGCt0QkXxC+Mvxq1YaF4P1W40zSpHy6W6AzODwcEc49yfwr3H9nv44/D39jnQT4X+InwouZ7pppJrjxp5SSSSbmJVJJGJkXaDjOAox24FfLXgP9oX9rr4CfHmP4QaF8KvCkVtfqj6Vq+uecy3asxHRJE5GMkcsBzg8V6h4+/ao/aq8SWVxpHxn/ZE0fVNKInR5/CkVxbzMsb7GkCTb9y56E4zkGurD06lCnzSerV9TX2yxF6bS31V7P7tLn0t44/4KsfAq88Pz6ZdanFAZICFmmJBAI7Adcda+Y4f2xoPAPxXSf4a+Nb/xN4O1XD3v9sRNJ/Zs7YLBXb70ZJYgE4UYA4r581bxP+yDbeLxB8UPgj440Jzslm/tXTdwWMthWwT0JBGe/TivYbnxp+x54g8Ewab8EfGtnLAjBTpjLskVyDuyjjI5z2x71liJ15R55brsaUqdCleMY2X3n1O/xn8MeI/DMUsE1q8ZjBUAKAoHOQew/Kvjv9uLxr4eudKmk0828oJ3eYkgO4jjATPA/wBoY/HNaXhnXJdD0N4tMuzJBb5SPz8uQMdCRx3r52/aM8Y/8JHdulzqUEUEbHzndtij8+Me5rCjiJ4qrGLQ68qOFwk5+RS/Yr/bXvtSgh+F/wASNQY3tsoWyvDlvPXuWJJy3X0Ffof+zzpui+PZ1Eu2V49rR5QOMY65FfhzqmmX1qyaxZmVJYG3xupwMjnp619Ofsc/8FFtb+F2pWNp4tvpmt7dwGnDgMBn7rcdPz6V9bn2Q/VsT7SmtH0PzbhriT6xh1SrS1Wl/wDM/W74xfCCy1rw2kK2cS3Nopa1meNghPplGBH1U5+tfPmvfGzx74b0a88Pap4p8UaRbbgiXFjbW1/Erx/LhVk2PGDg9WYnrXt3wX/bn+EnxY8OW9//AGtazxzxhVZSOW9Mdzz7Yrc1zxf8F/Ec7t/ZFhcvDguLlQSnH90HP+NeH9ZlTXLdejR+lYPFSdBQnTjUj0uk7ej3R8YfFHxr8UfjRZSxeB9Pd5riCFG1C/8ABsKyN5b7l3u0rDrzgqMdRXA+EP2APDfhTVm+KnxO1+e78RTFpreK3lESxOxyW2x4HXj055Br7j1jWPhVoyyNY6JYxHPDkgID6AE9utfOn7SP7U3wf+EkD32teL7GK4lB8oSTLyO+xcEnB44BrjnXnVbjTSu9Pdvdl4r6ioKbowppa3/XVtL5WOK8dnSvh34Mm0eyZlupiTP5k53b+4J9cY4Ir5SPxQ+F2u/HDSfCHxG8Z2th4chu/M1/UL2wa7gK94Skcil+vIDqRtrn/jX+3f4l+Ini630z4S6DLdSRXqzfaLy2837TtYNtEWMbDjkkcgngda+jLj9kTxd+37ZeHfH3xA0Gw8DW+kaOYv7C8H/D+K0e7lYoTI0rTHcHILNK5YoTtSIqfl+gyLJfq/7+u7PpF/n/AMA+Dz7O/rz+q4NOUeslt6I//9k=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated Age: 29\n",
      "Estimated Gender: female\n",
      "Estimated Emotion: angry\n",
      "Original Image Coordinates:[0.7024455  0.3754582  0.77637887 0.5739976 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated Age: 24\n",
      "Estimated Gender: male\n",
      "Estimated Emotion: happy\n",
      "Original Image Coordinates:[0.16671757 0.31987876 0.23432626 0.50335705]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated Age: 22\n",
      "Estimated Gender: female\n",
      "Estimated Emotion: happy\n",
      "Original Image Coordinates:[0.7956609  0.386524   0.85938865 0.5990172 ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated Age: 32\n",
      "Estimated Gender: male\n",
      "Estimated Emotion: happy\n",
      "Original Image Coordinates:[0.5632465  0.37932324 0.61377335 0.57810456]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated Age: 21\n",
      "Estimated Gender: female\n",
      "Estimated Emotion: happy\n",
      "Original Image Coordinates:[0.9035787  0.31419483 0.9603891  0.52992076]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nFound', len(people), 'faces:\\n')\n",
    "for num, person in enumerate(people):\n",
    "    display(Image(images[num]))\n",
    "    print(f\"\"\"\n",
    "Estimated Age: {person['age']}\n",
    "Estimated Gender: {person['gender']}\n",
    "Estimated Emotion: {person['emotion']}\n",
    "Original Image Coordinates:{person['coordinate']}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99576a46",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Let's free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77ed5400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ovms.intel.com \"ovms-pipeline\" deleted\n",
      "deployment.apps \"minio\" deleted\n",
      "service \"minio-service\" deleted\n",
      "configmap \"ovms-face-detection-pipeline\" deleted\n"
     ]
    }
   ],
   "source": [
    "!oc delete ovms ovms-pipeline\n",
    "!oc delete deploy minio\n",
    "!oc delete service minio-service\n",
    "!oc delete configmap ovms-face-detection-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad6fe4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E0512 16:36:11.541914476   11783 backup_poller.cc:133]       Run client channel backup poller: {\"created\":\"@1620837371.541887269\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":321,\"referenced_errors\":[{\"created\":\"@1620837371.541886155\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":948,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n",
      "E0512 16:36:12.541738498   11799 ev_epollex_linux.cc:1365]   pollset_set_add_fd: {\"created\":\"@1620837372.541698898\",\"description\":\"pollset_set_add_fd\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":321,\"referenced_errors\":[{\"created\":\"@1620837372.541698058\",\"description\":\"pollable_add_fd\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":321,\"referenced_errors\":[{\"created\":\"@1620837372.541696844\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":631,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_ctl\"}]},{\"created\":\"@1620837372.541703409\",\"description\":\"pollable_add_fd\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":321,\"referenced_errors\":[{\"created\":\"@1620837372.541700230\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":631,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_ctl\"}]}]}\n",
      "E0512 16:36:12.541795328   11799 ev_epollex_linux.cc:516]    Error shutting down fd 50. errno: 9\n",
      "E0512 16:36:13.541769447   11815 ev_epollex_linux.cc:1365]   pollset_set_add_fd: {\"created\":\"@1620837373.541729203\",\"description\":\"pollset_set_add_fd\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":321,\"referenced_errors\":[{\"created\":\"@1620837373.541728448\",\"description\":\"pollable_add_fd\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":321,\"referenced_errors\":[{\"created\":\"@1620837373.541727009\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":631,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_ctl\"}]},{\"created\":\"@1620837373.541733125\",\"description\":\"pollable_add_fd\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":321,\"referenced_errors\":[{\"created\":\"@1620837373.541730440\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":631,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_ctl\"}]}]}\n",
      "E0512 16:36:13.541826983   11815 ev_epollex_linux.cc:516]    Error shutting down fd 50. errno: 9\n"
     ]
    }
   ],
   "source": [
    "!rm -rf age-gender\n",
    "!rm -rf emotions\n",
    "!rm -rf face-detection\n",
    "!rm -rf python\n",
    "!rm -rf opencv\n",
    "!rm -rf model_server\n",
    "!rm mc\n",
    "!rm face_images_*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e6c19f",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In this notebook, you have learned how to deploy an OVMS service with multiple models in an OpenShift cluster. Next, you can explore other OVMS notebooks:\n",
    "\n",
    "- [Deploy Image Classification with OpenVINO Model Server in OpenShift](../401-model-serving-openshift-resnet/ovms-openshift-resnet.ipynb)\n",
    "- [Send gRPC and API Calls via Python Scripts to OpenVINO Model Server in OpenShift](../402-model-serving-openshift-python-scripts/ovms-openshift-python-scripts.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
